{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wUHhr-z1g0P8"
   },
   "source": [
    "<h1> Local implementation </h1>\n",
    "    \n",
    "https://github.com/minimaxir/gpt-2-simple/blob/master/gpt_2_simple/gpt_2.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Package code below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "import json\n",
    "import re\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.core.protobuf import rewriter_config_pb2\n",
    "from datetime import datetime\n",
    "\n",
    "import model, sample, encoder # from local files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def start_tf_sess(threads=-1, server=None):\n",
    "    \"\"\"\n",
    "    Returns a tf.Session w/ config\n",
    "    \"\"\"\n",
    "    config = tf.compat.v1.ConfigProto()\n",
    "    config.gpu_options.allow_growth = True\n",
    "    config.graph_options.rewrite_options.layout_optimizer = rewriter_config_pb2.RewriterConfig.OFF\n",
    "    if threads > 0:\n",
    "        config.intra_op_parallelism_threads = threads\n",
    "        config.inter_op_parallelism_threads = threads\n",
    "\n",
    "    if server is not None:\n",
    "        return tf.compat.v1.Session(target=server.target, config=config)\n",
    "    \n",
    "    return tf.compat.v1.Session(config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_gpt2(sess,\n",
    "              checkpoint='latest',\n",
    "              run_name=\"run1\",\n",
    "              checkpoint_dir=\"checkpoint\",\n",
    "              model_name=None,\n",
    "              model_dir='models',\n",
    "              multi_gpu=False):\n",
    "    \"\"\"Loads the model checkpoint or existing model into a TensorFlow session\n",
    "    for repeated predictions.\n",
    "    \"\"\"\n",
    "\n",
    "    if model_name:\n",
    "        checkpoint_path = os.path.join(model_dir, model_name)\n",
    "    else:\n",
    "        checkpoint_path = os.path.join(checkpoint_dir, run_name)\n",
    "\n",
    "    hparams = model.default_hparams()\n",
    "    with open(os.path.join(checkpoint_path, 'hparams.json')) as f:\n",
    "        hparams.override_from_dict(json.load(f))\n",
    "\n",
    "    context = tf.compat.v1.placeholder(tf.int32, [1, None])\n",
    "\n",
    "    gpus = []\n",
    "    if multi_gpu:\n",
    "        gpus = get_available_gpus()\n",
    "\n",
    "    output = model.model(hparams=hparams, X=context, gpus=gpus)\n",
    "\n",
    "    if checkpoint=='latest':\n",
    "        ckpt = tf.train.latest_checkpoint(checkpoint_path)\n",
    "    else:\n",
    "        ckpt = os.path.join(checkpoint_path,checkpoint)\n",
    "\n",
    "    saver = tf.compat.v1.train.Saver(allow_empty=True)\n",
    "    sess.run(tf.compat.v1.global_variables_initializer())\n",
    "\n",
    "    if model_name:\n",
    "        print('Loading pretrained model', ckpt)\n",
    "    else:\n",
    "        print('Loading checkpoint', ckpt)\n",
    "    saver.restore(sess, ckpt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(sess,\n",
    "             run_name='run1',\n",
    "             checkpoint_dir='checkpoint',\n",
    "             model_name=None,\n",
    "             model_dir='models',\n",
    "             sample_dir='samples',\n",
    "             return_as_list=False,\n",
    "             truncate=None,\n",
    "             destination_path=None,\n",
    "             sample_delim='=' * 20 + '\\n',\n",
    "             prefix=None,\n",
    "             seed=None,\n",
    "             nsamples=1,\n",
    "             batch_size=1,\n",
    "             length=1023,\n",
    "             temperature=0.7,\n",
    "             top_k=0,\n",
    "             top_p=0.0,\n",
    "             include_prefix=True):\n",
    "    \"\"\"Generates text from a model loaded into memory.\n",
    "    Adapted from https://github.com/openai/gpt-2/blob/master/src/interactive_conditional_samples.py\n",
    "    \"\"\"\n",
    "\n",
    "    if batch_size is None:\n",
    "        batch_size = 1\n",
    "    assert nsamples % batch_size == 0\n",
    "\n",
    "    if nsamples == 1:\n",
    "        sample_delim = ''\n",
    "\n",
    "    if prefix == '':\n",
    "        prefix = None\n",
    "\n",
    "    if model_name:\n",
    "        checkpoint_path = os.path.join(model_dir, model_name)\n",
    "    else:\n",
    "        checkpoint_path = os.path.join(checkpoint_dir, run_name)\n",
    "\n",
    "    enc = encoder.get_encoder(checkpoint_path)\n",
    "    hparams = model.default_hparams()\n",
    "    with open(os.path.join(checkpoint_path, 'hparams.json')) as f:\n",
    "        hparams.override_from_dict(json.load(f))\n",
    "\n",
    "    if prefix:\n",
    "        context = tf.compat.v1.placeholder(tf.int32, [batch_size, None])\n",
    "        context_tokens = enc.encode(prefix)\n",
    "\n",
    "    np.random.seed(seed)\n",
    "    tf.compat.v1.set_random_seed(seed)\n",
    "\n",
    "    output = sample.sample_sequence(\n",
    "        hparams=hparams,\n",
    "        length=min(length, 1023 - (len(context_tokens) if prefix else 0)),\n",
    "        start_token=enc.encoder['<|endoftext|>'] if not prefix else None,\n",
    "        context=context if prefix else None,\n",
    "        batch_size=batch_size,\n",
    "        temperature=temperature, top_k=top_k, top_p=top_p\n",
    "    )[:, 1:]\n",
    "\n",
    "    if destination_path:\n",
    "        f = open(destination_path, 'w')\n",
    "    generated = 0\n",
    "    gen_texts = []\n",
    "    while generated < nsamples:\n",
    "        if not prefix:\n",
    "            out = sess.run(output)\n",
    "        else:\n",
    "            out = sess.run(output, feed_dict={\n",
    "                    context: batch_size * [context_tokens]\n",
    "                })\n",
    "        for i in range(batch_size):\n",
    "            generated += 1\n",
    "            gen_text = enc.decode(out[i])\n",
    "            if prefix:\n",
    "                gen_text = enc.decode(context_tokens[:1]) + gen_text\n",
    "            if truncate:\n",
    "                truncate_esc = re.escape(truncate)\n",
    "                if prefix and not include_prefix:\n",
    "                    prefix_esc = re.escape(prefix)\n",
    "                    pattern = '(?:{})(.*?)(?:{})'.format(prefix_esc,\n",
    "                                                         truncate_esc)\n",
    "                else:\n",
    "                    pattern = '(.*?)(?:{})'.format(truncate_esc)\n",
    "\n",
    "                trunc_text = re.search(pattern, gen_text, re.S)\n",
    "                if trunc_text:\n",
    "                    gen_text = trunc_text.group(1)\n",
    "            gen_text = gen_text.lstrip('\\n')\n",
    "            if destination_path:\n",
    "                f.write(\"{}\\n{}\".format(gen_text, sample_delim))\n",
    "            if not return_as_list and not destination_path:\n",
    "                print(\"{}\\n{}\".format(gen_text, sample_delim), end='')\n",
    "            gen_texts.append(gen_text)\n",
    "\n",
    "    if destination_path:\n",
    "        f.close()\n",
    "\n",
    "    if return_as_list:\n",
    "        return gen_texts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Run model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 158
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 59414,
     "status": "ok",
     "timestamp": 1598508788420,
     "user": {
      "displayName": "Ryan L",
      "photoUrl": "",
      "userId": "11972670487865846176"
     },
     "user_tz": 240
    },
    "id": "-p0hc7e-c3Sh",
    "outputId": "bca9c462-dbaa-4ce4-930c-3e581a366200"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Ryan\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "Loading checkpoint checkpoint\\run1\\model-200\n",
      "WARNING:tensorflow:From C:\\Users\\Ryan\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\training\\saver.py:1266: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use standard file APIs to check for files with this prefix.\n",
      "INFO:tensorflow:Restoring parameters from checkpoint\\run1\\model-200\n"
     ]
    }
   ],
   "source": [
    "sess = start_tf_sess()\n",
    "load_gpt2(sess, run_name='run1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 69
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 11250,
     "status": "ok",
     "timestamp": 1598508873696,
     "user": {
      "displayName": "Ryan L",
      "photoUrl": "",
      "userId": "11972670487865846176"
     },
     "user_tz": 240
    },
    "id": "Kdox4ahHc7rx",
    "outputId": "7df7cb4c-9c83-4b93-bd77-84f6583036e5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \"You may be tempted to put the blame for what has been a long-term problem on the shoulders of a stranger, but the fact is that they don't – they have no point. No money to spend on a new project or a new job, and no other obligations at the moment. It would be so much fun if you could make a difference and take part in something so magical. \n"
     ]
    }
   ],
   "source": [
    "pre = \"\\\"\" + \"gemini 09-01-2020\" + \"\\\"\"\n",
    "\n",
    "generate(sess, run_name='run1', prefix = pre,\n",
    "              truncate = \"<end>\", include_prefix = False, # hide start/end\n",
    "              length = 200) # default 1023 is a bit too long"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyM8SvDME2/tCT2ICt91w2gJ",
   "collapsed_sections": [],
   "name": "modified.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
